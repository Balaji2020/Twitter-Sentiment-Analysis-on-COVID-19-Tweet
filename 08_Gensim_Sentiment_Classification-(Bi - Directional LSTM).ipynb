{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srmetlakunta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714025946899135\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence \n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "print(random.random())\n",
    "\n",
    "### Deep learning library\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout,SpatialDropout1D, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_date</th>\n",
       "      <th>Tweet_time</th>\n",
       "      <th>Tweet_City</th>\n",
       "      <th>Tweet_Country</th>\n",
       "      <th>Tweet_account</th>\n",
       "      <th>Retweet_count</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>tweet_without_stopwords</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4/1/2020</td>\n",
       "      <td>0:08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>GSK_AU</td>\n",
       "      <td>0</td>\n",
       "      <td>ask award research excellence open nomination ...</td>\n",
       "      <td>2020-04-01 00:08:00</td>\n",
       "      <td>ask award research excellence open nomination ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.9349</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4/1/2020</td>\n",
       "      <td>0:35:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>GSK_AU</td>\n",
       "      <td>3</td>\n",
       "      <td>award research excellence open nomination awar...</td>\n",
       "      <td>2020-04-01 00:35:00</td>\n",
       "      <td>award research excellence open nomination awar...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tweet_date Tweet_time Tweet_City Tweet_Country Tweet_account  Retweet_count  \\\n",
       "0   4/1/2020    0:08:00        NaN     Australia        GSK_AU              0   \n",
       "1   4/1/2020    0:35:00        NaN     Australia        GSK_AU              3   \n",
       "\n",
       "                                          Tweet_Text         Created Date  \\\n",
       "0  ask award research excellence open nomination ...  2020-04-01 00:08:00   \n",
       "1  award research excellence open nomination awar...  2020-04-01 00:35:00   \n",
       "\n",
       "                             tweet_without_stopwords  neg    neu    pos  \\\n",
       "0  ask award research excellence open nomination ...  0.0  0.297  0.703   \n",
       "1  award research excellence open nomination awar...  0.0  0.419  0.581   \n",
       "\n",
       "   vader_polarity sentiment  \n",
       "0          0.9349  positive  \n",
       "1          0.9022  positive  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/sentiment_twitter_data.csv\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['tweet_without_stopwords']\n",
    "y = data['sentiment'].apply({'positive':2,'negative':0,'neutral':1}.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "progress-bar:   0%|                                                                          | 0/13724 [00:00<?, ?it/s]\n",
      "progress-bar:  12%|███████                                                      | 1580/13724 [00:00<00:05, 2330.72it/s]\n",
      "progress-bar:  21%|████████████▌                                                | 2837/13724 [00:00<00:03, 3082.83it/s]\n",
      "progress-bar:  32%|███████████████████▎                                         | 4345/13724 [00:00<00:02, 4046.90it/s]\n",
      "progress-bar:  44%|██████████████████████████▉                                  | 6049/13724 [00:00<00:01, 5245.78it/s]\n",
      "progress-bar:  55%|█████████████████████████████████▎                           | 7498/13724 [00:01<00:00, 6484.80it/s]\n",
      "progress-bar:  64%|███████████████████████████████████████▎                     | 8844/13724 [00:01<00:00, 7671.29it/s]\n",
      "progress-bar:  73%|███████████████████████████████████████████▉                | 10055/13724 [00:01<00:00, 8568.51it/s]\n",
      "progress-bar:  82%|█████████████████████████████████████████████████▏          | 11256/13724 [00:01<00:00, 9335.84it/s]\n",
      "progress-bar: 100%|████████████████████████████████████████████████████████████| 13724/13724 [00:01<00:00, 8940.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'\n",
    "\n",
    "def postprocess(data, n=300):\n",
    "    data['tokens'] = data['Tweet_Text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    # data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "tokenData = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_date</th>\n",
       "      <th>Tweet_time</th>\n",
       "      <th>Tweet_City</th>\n",
       "      <th>Tweet_Country</th>\n",
       "      <th>Tweet_account</th>\n",
       "      <th>Retweet_count</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>tweet_without_stopwords</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4/1/2020</td>\n",
       "      <td>0:08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>GSK_AU</td>\n",
       "      <td>0</td>\n",
       "      <td>ask award research excellence open nomination ...</td>\n",
       "      <td>2020-04-01 00:08:00</td>\n",
       "      <td>ask award research excellence open nomination ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.9349</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ask, award, research, excellence, open, nomin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tweet_date Tweet_time Tweet_City Tweet_Country Tweet_account  Retweet_count  \\\n",
       "0   4/1/2020    0:08:00        NaN     Australia        GSK_AU              0   \n",
       "\n",
       "                                          Tweet_Text         Created Date  \\\n",
       "0  ask award research excellence open nomination ...  2020-04-01 00:08:00   \n",
       "\n",
       "                             tweet_without_stopwords  neg    neu    pos  \\\n",
       "0  ask award research excellence open nomination ...  0.0  0.297  0.703   \n",
       "\n",
       "   vader_polarity sentiment                                             tokens  \n",
       "0          0.9349  positive  [ask, award, research, excellence, open, nomin...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenData.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare custom fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████| 13724/13724 [00:00<00:00, 1058603.58it/s]\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13724/13724 [00:00<00:00, 611074.73it/s]\n"
     ]
    }
   ],
   "source": [
    "f2vec = FastText(size=300, window=5, min_count=3, workers=4,sg=1)\n",
    "f2vec.build_vocab([x for x in tqdm(data['tokens'])])\n",
    "f2vec.train([x for x in tqdm(data['tokens'])],total_examples=f2vec.corpus_count,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10979,), (10979,), (2745,), (2745,), (10979,), (2745,))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x, train_y,test_y, y1,y2 = train_test_split(data['Tweet_Text'], y,data['sentiment'], test_size=0.2,\n",
    "                                                          random_state=1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data for deep learning architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/13724 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 13724/13724 [00:00<00:00, 72873.86it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = len(x_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "def text_to_wordlist(text, lower=False):\n",
    "    # Return a list of words\n",
    "    vocab.update(text)\n",
    "    return text\n",
    "\n",
    "def process_comments(list_sentences, lower=False):\n",
    "    comments = []\n",
    "    for text in tqdm(list_sentences):\n",
    "        txt = text_to_wordlist(text, lower=lower)\n",
    "        comments.append(txt)\n",
    "    return comments\n",
    "\n",
    "list_sentences_train = list(train_x.values)\n",
    "list_sentences_test = list(test_x.values)\n",
    "\n",
    "comments = process_comments(list_sentences_train + list_sentences_test, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10979, 200)\n",
      "Shape of label tensor: (10979,)\n",
      "Shape of test_data tensor: (2745, 200)\n"
     ]
    }
   ],
   "source": [
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "sequences = [[word_index.get(t, 0) for t in comment]\n",
    "             for comment in comments[:len(list_sentences_train)]]\n",
    "test_sequences = [[word_index.get(t, 0)  for t in comment] \n",
    "                  for comment in comments[len(list_sentences_train):]]\n",
    "\n",
    "#pad\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "list_classes = [\"positive\", \"negative\", \"neutral\"]\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y1.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                          truncating=\"post\")\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "WV_DIM = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(x_vectors.vocab))\n",
    "# we initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = x_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_layer = Embedding(nb_words,\n",
    "                     WV_DIM,\n",
    "                     mask_zero=False,\n",
    "                     weights=[wv_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=False)\n",
    "\n",
    "# Inputs\n",
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = wv_layer(comment_input)\n",
    "\n",
    "# biGRU\n",
    "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n",
    "\n",
    "# Output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(3, activation='sigmoid')(x)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=[comment_input], outputs=preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9881 samples, validate on 1098 samples\n",
      "Epoch 1/10\n",
      "9881/9881 [==============================] - ETA: 9:22 - loss: 1.2384 - accuracy: 0.25 - ETA: 7:04 - loss: 1.2335 - accuracy: 0.28 - ETA: 6:30 - loss: 1.2072 - accuracy: 0.29 - ETA: 6:03 - loss: 1.2023 - accuracy: 0.30 - ETA: 5:45 - loss: 1.1912 - accuracy: 0.31 - ETA: 5:27 - loss: 1.1800 - accuracy: 0.32 - ETA: 5:12 - loss: 1.1739 - accuracy: 0.32 - ETA: 4:58 - loss: 1.1714 - accuracy: 0.33 - ETA: 4:44 - loss: 1.1670 - accuracy: 0.33 - ETA: 4:31 - loss: 1.1625 - accuracy: 0.33 - ETA: 4:20 - loss: 1.1560 - accuracy: 0.34 - ETA: 4:09 - loss: 1.1474 - accuracy: 0.35 - ETA: 3:58 - loss: 1.1447 - accuracy: 0.35 - ETA: 3:49 - loss: 1.1408 - accuracy: 0.36 - ETA: 3:40 - loss: 1.1368 - accuracy: 0.36 - ETA: 3:29 - loss: 1.1348 - accuracy: 0.36 - ETA: 3:20 - loss: 1.1298 - accuracy: 0.37 - ETA: 3:11 - loss: 1.1249 - accuracy: 0.37 - ETA: 3:02 - loss: 1.1255 - accuracy: 0.37 - ETA: 2:52 - loss: 1.1221 - accuracy: 0.38 - ETA: 2:43 - loss: 1.1196 - accuracy: 0.38 - ETA: 2:34 - loss: 1.1174 - accuracy: 0.38 - ETA: 2:25 - loss: 1.1152 - accuracy: 0.39 - ETA: 2:16 - loss: 1.1116 - accuracy: 0.39 - ETA: 2:06 - loss: 1.1092 - accuracy: 0.39 - ETA: 1:57 - loss: 1.1072 - accuracy: 0.39 - ETA: 1:48 - loss: 1.1051 - accuracy: 0.39 - ETA: 1:39 - loss: 1.1042 - accuracy: 0.39 - ETA: 1:30 - loss: 1.1014 - accuracy: 0.40 - ETA: 1:20 - loss: 1.0989 - accuracy: 0.40 - ETA: 1:11 - loss: 1.0980 - accuracy: 0.40 - ETA: 1:02 - loss: 1.0952 - accuracy: 0.40 - ETA: 53s - loss: 1.0940 - accuracy: 0.4062 - ETA: 43s - loss: 1.0931 - accuracy: 0.407 - ETA: 34s - loss: 1.0917 - accuracy: 0.408 - ETA: 24s - loss: 1.0902 - accuracy: 0.410 - ETA: 15s - loss: 1.0886 - accuracy: 0.412 - ETA: 5s - loss: 1.0859 - accuracy: 0.415 - 380s 38ms/sample - loss: 1.0855 - accuracy: 0.4165 - val_loss: 1.0828 - val_accuracy: 0.4381\n",
      "Epoch 2/10\n",
      "9881/9881 [==============================] - ETA: 6:11 - loss: 0.9923 - accuracy: 0.51 - ETA: 5:57 - loss: 1.0252 - accuracy: 0.46 - ETA: 5:55 - loss: 1.0356 - accuracy: 0.46 - ETA: 5:48 - loss: 1.0347 - accuracy: 0.46 - ETA: 5:33 - loss: 1.0287 - accuracy: 0.46 - ETA: 5:25 - loss: 1.0257 - accuracy: 0.46 - ETA: 5:13 - loss: 1.0253 - accuracy: 0.46 - ETA: 5:03 - loss: 1.0267 - accuracy: 0.46 - ETA: 4:53 - loss: 1.0291 - accuracy: 0.46 - ETA: 4:44 - loss: 1.0296 - accuracy: 0.46 - ETA: 4:34 - loss: 1.0279 - accuracy: 0.46 - ETA: 4:26 - loss: 1.0267 - accuracy: 0.46 - ETA: 4:17 - loss: 1.0287 - accuracy: 0.46 - ETA: 4:06 - loss: 1.0265 - accuracy: 0.46 - ETA: 3:55 - loss: 1.0249 - accuracy: 0.46 - ETA: 3:46 - loss: 1.0235 - accuracy: 0.46 - ETA: 3:35 - loss: 1.0221 - accuracy: 0.46 - ETA: 3:25 - loss: 1.0221 - accuracy: 0.46 - ETA: 3:15 - loss: 1.0229 - accuracy: 0.46 - ETA: 3:04 - loss: 1.0227 - accuracy: 0.46 - ETA: 2:54 - loss: 1.0221 - accuracy: 0.46 - ETA: 2:45 - loss: 1.0204 - accuracy: 0.47 - ETA: 2:36 - loss: 1.0199 - accuracy: 0.47 - ETA: 2:27 - loss: 1.0185 - accuracy: 0.47 - ETA: 2:17 - loss: 1.0175 - accuracy: 0.47 - ETA: 2:07 - loss: 1.0164 - accuracy: 0.47 - ETA: 1:56 - loss: 1.0187 - accuracy: 0.47 - ETA: 1:46 - loss: 1.0186 - accuracy: 0.47 - ETA: 1:36 - loss: 1.0192 - accuracy: 0.47 - ETA: 1:27 - loss: 1.0177 - accuracy: 0.47 - ETA: 1:17 - loss: 1.0157 - accuracy: 0.47 - ETA: 1:07 - loss: 1.0163 - accuracy: 0.47 - ETA: 57s - loss: 1.0159 - accuracy: 0.4734 - ETA: 47s - loss: 1.0153 - accuracy: 0.473 - ETA: 36s - loss: 1.0146 - accuracy: 0.473 - ETA: 26s - loss: 1.0141 - accuracy: 0.473 - ETA: 16s - loss: 1.0149 - accuracy: 0.473 - ETA: 6s - loss: 1.0140 - accuracy: 0.473 - 404s 41ms/sample - loss: 1.0147 - accuracy: 0.4737 - val_loss: 1.0747 - val_accuracy: 0.3953\n",
      "Epoch 3/10\n",
      "9881/9881 [==============================] - ETA: 7:04 - loss: 0.9834 - accuracy: 0.51 - ETA: 7:01 - loss: 0.9995 - accuracy: 0.50 - ETA: 6:52 - loss: 0.9950 - accuracy: 0.51 - ETA: 6:36 - loss: 1.0058 - accuracy: 0.50 - ETA: 6:24 - loss: 0.9944 - accuracy: 0.51 - ETA: 6:12 - loss: 0.9967 - accuracy: 0.51 - ETA: 6:01 - loss: 0.9974 - accuracy: 0.50 - ETA: 5:44 - loss: 1.0033 - accuracy: 0.50 - ETA: 5:33 - loss: 1.0043 - accuracy: 0.50 - ETA: 5:20 - loss: 0.9990 - accuracy: 0.50 - ETA: 5:09 - loss: 0.9984 - accuracy: 0.49 - ETA: 4:57 - loss: 0.9934 - accuracy: 0.50 - ETA: 4:46 - loss: 0.9948 - accuracy: 0.50 - ETA: 4:35 - loss: 0.9931 - accuracy: 0.50 - ETA: 4:25 - loss: 0.9910 - accuracy: 0.50 - ETA: 4:14 - loss: 0.9923 - accuracy: 0.50 - ETA: 4:04 - loss: 0.9909 - accuracy: 0.50 - ETA: 3:52 - loss: 0.9893 - accuracy: 0.50 - ETA: 3:41 - loss: 0.9901 - accuracy: 0.50 - ETA: 3:30 - loss: 0.9904 - accuracy: 0.50 - ETA: 3:18 - loss: 0.9898 - accuracy: 0.50 - ETA: 3:07 - loss: 0.9898 - accuracy: 0.50 - ETA: 2:56 - loss: 0.9894 - accuracy: 0.50 - ETA: 2:44 - loss: 0.9910 - accuracy: 0.50 - ETA: 2:33 - loss: 0.9907 - accuracy: 0.49 - ETA: 2:22 - loss: 0.9891 - accuracy: 0.50 - ETA: 2:11 - loss: 0.9879 - accuracy: 0.50 - ETA: 1:59 - loss: 0.9883 - accuracy: 0.50 - ETA: 1:47 - loss: 0.9895 - accuracy: 0.50 - ETA: 1:36 - loss: 0.9908 - accuracy: 0.50 - ETA: 1:25 - loss: 0.9918 - accuracy: 0.50 - ETA: 1:14 - loss: 0.9903 - accuracy: 0.50 - ETA: 1:02 - loss: 0.9908 - accuracy: 0.50 - ETA: 51s - loss: 0.9908 - accuracy: 0.4994 - ETA: 40s - loss: 0.9900 - accuracy: 0.499 - ETA: 29s - loss: 0.9891 - accuracy: 0.500 - ETA: 17s - loss: 0.9891 - accuracy: 0.500 - ETA: 6s - loss: 0.9889 - accuracy: 0.500 - 436s 44ms/sample - loss: 0.9886 - accuracy: 0.5007 - val_loss: 1.0684 - val_accuracy: 0.4681\n",
      "Epoch 4/10\n",
      "9881/9881 [==============================] - ETA: 7:13 - loss: 0.9913 - accuracy: 0.44 - ETA: 6:51 - loss: 0.9708 - accuracy: 0.47 - ETA: 6:46 - loss: 0.9756 - accuracy: 0.47 - ETA: 6:32 - loss: 0.9620 - accuracy: 0.49 - ETA: 6:17 - loss: 0.9700 - accuracy: 0.49 - ETA: 6:08 - loss: 0.9742 - accuracy: 0.49 - ETA: 5:57 - loss: 0.9769 - accuracy: 0.49 - ETA: 5:45 - loss: 0.9749 - accuracy: 0.50 - ETA: 5:34 - loss: 0.9667 - accuracy: 0.50 - ETA: 5:21 - loss: 0.9604 - accuracy: 0.51 - ETA: 5:10 - loss: 0.9611 - accuracy: 0.50 - ETA: 4:59 - loss: 0.9626 - accuracy: 0.50 - ETA: 4:49 - loss: 0.9633 - accuracy: 0.50 - ETA: 4:37 - loss: 0.9681 - accuracy: 0.50 - ETA: 4:25 - loss: 0.9700 - accuracy: 0.50 - ETA: 4:12 - loss: 0.9679 - accuracy: 0.50 - ETA: 4:00 - loss: 0.9682 - accuracy: 0.50 - ETA: 3:50 - loss: 0.9692 - accuracy: 0.50 - ETA: 3:39 - loss: 0.9698 - accuracy: 0.50 - ETA: 3:29 - loss: 0.9693 - accuracy: 0.51 - ETA: 3:17 - loss: 0.9690 - accuracy: 0.51 - ETA: 3:06 - loss: 0.9683 - accuracy: 0.51 - ETA: 2:55 - loss: 0.9670 - accuracy: 0.51 - ETA: 2:43 - loss: 0.9682 - accuracy: 0.51 - ETA: 2:31 - loss: 0.9680 - accuracy: 0.51 - ETA: 2:19 - loss: 0.9687 - accuracy: 0.51 - ETA: 2:08 - loss: 0.9676 - accuracy: 0.51 - ETA: 1:57 - loss: 0.9697 - accuracy: 0.51 - ETA: 1:46 - loss: 0.9686 - accuracy: 0.51 - ETA: 1:36 - loss: 0.9687 - accuracy: 0.51 - ETA: 1:25 - loss: 0.9675 - accuracy: 0.51 - ETA: 1:14 - loss: 0.9671 - accuracy: 0.51 - ETA: 1:03 - loss: 0.9656 - accuracy: 0.51 - ETA: 51s - loss: 0.9654 - accuracy: 0.5152 - ETA: 40s - loss: 0.9666 - accuracy: 0.514 - ETA: 29s - loss: 0.9665 - accuracy: 0.514 - ETA: 18s - loss: 0.9662 - accuracy: 0.513 - ETA: 6s - loss: 0.9662 - accuracy: 0.513 - 441s 45ms/sample - loss: 0.9656 - accuracy: 0.5141 - val_loss: 1.0554 - val_accuracy: 0.4836\n",
      "Epoch 5/10\n",
      "9881/9881 [==============================] - ETA: 7:18 - loss: 0.9502 - accuracy: 0.54 - ETA: 7:15 - loss: 0.9275 - accuracy: 0.56 - ETA: 7:01 - loss: 0.9356 - accuracy: 0.54 - ETA: 6:36 - loss: 0.9471 - accuracy: 0.53 - ETA: 6:29 - loss: 0.9619 - accuracy: 0.52 - ETA: 6:18 - loss: 0.9661 - accuracy: 0.51 - ETA: 6:07 - loss: 0.9687 - accuracy: 0.52 - ETA: 5:55 - loss: 0.9677 - accuracy: 0.51 - ETA: 5:43 - loss: 0.9662 - accuracy: 0.51 - ETA: 5:31 - loss: 0.9659 - accuracy: 0.51 - ETA: 5:21 - loss: 0.9614 - accuracy: 0.51 - ETA: 5:03 - loss: 0.9604 - accuracy: 0.51 - ETA: 4:49 - loss: 0.9621 - accuracy: 0.51 - ETA: 4:37 - loss: 0.9609 - accuracy: 0.51 - ETA: 4:26 - loss: 0.9584 - accuracy: 0.52 - ETA: 4:15 - loss: 0.9571 - accuracy: 0.52 - ETA: 4:05 - loss: 0.9571 - accuracy: 0.52 - ETA: 3:54 - loss: 0.9580 - accuracy: 0.52 - ETA: 3:43 - loss: 0.9578 - accuracy: 0.52 - ETA: 3:31 - loss: 0.9545 - accuracy: 0.52 - ETA: 3:20 - loss: 0.9549 - accuracy: 0.51 - ETA: 3:09 - loss: 0.9527 - accuracy: 0.52 - ETA: 2:58 - loss: 0.9512 - accuracy: 0.52 - ETA: 2:46 - loss: 0.9485 - accuracy: 0.52 - ETA: 2:35 - loss: 0.9484 - accuracy: 0.52 - ETA: 2:23 - loss: 0.9502 - accuracy: 0.52 - ETA: 2:12 - loss: 0.9492 - accuracy: 0.52 - ETA: 2:00 - loss: 0.9491 - accuracy: 0.52 - ETA: 1:49 - loss: 0.9488 - accuracy: 0.52 - ETA: 1:37 - loss: 0.9475 - accuracy: 0.52 - ETA: 1:26 - loss: 0.9492 - accuracy: 0.52 - ETA: 1:14 - loss: 0.9492 - accuracy: 0.52 - ETA: 1:03 - loss: 0.9486 - accuracy: 0.52 - ETA: 52s - loss: 0.9471 - accuracy: 0.5257 - ETA: 40s - loss: 0.9464 - accuracy: 0.526 - ETA: 29s - loss: 0.9473 - accuracy: 0.526 - ETA: 18s - loss: 0.9470 - accuracy: 0.528 - ETA: 6s - loss: 0.9472 - accuracy: 0.528 - 442s 45ms/sample - loss: 0.9473 - accuracy: 0.5285 - val_loss: 1.0375 - val_accuracy: 0.4973\n",
      "Epoch 6/10\n",
      "9881/9881 [==============================] - ETA: 6:47 - loss: 0.8872 - accuracy: 0.55 - ETA: 6:48 - loss: 0.9100 - accuracy: 0.52 - ETA: 6:28 - loss: 0.9204 - accuracy: 0.52 - ETA: 6:17 - loss: 0.9255 - accuracy: 0.52 - ETA: 6:09 - loss: 0.9271 - accuracy: 0.51 - ETA: 5:58 - loss: 0.9226 - accuracy: 0.52 - ETA: 5:46 - loss: 0.9169 - accuracy: 0.53 - ETA: 5:37 - loss: 0.9163 - accuracy: 0.54 - ETA: 5:25 - loss: 0.9140 - accuracy: 0.54 - ETA: 5:17 - loss: 0.9137 - accuracy: 0.54 - ETA: 5:06 - loss: 0.9198 - accuracy: 0.54 - ETA: 4:53 - loss: 0.9191 - accuracy: 0.54 - ETA: 4:38 - loss: 0.9201 - accuracy: 0.54 - ETA: 4:27 - loss: 0.9226 - accuracy: 0.54 - ETA: 4:17 - loss: 0.9253 - accuracy: 0.53 - ETA: 4:06 - loss: 0.9262 - accuracy: 0.53 - ETA: 3:55 - loss: 0.9241 - accuracy: 0.54 - ETA: 3:45 - loss: 0.9256 - accuracy: 0.53 - ETA: 3:33 - loss: 0.9242 - accuracy: 0.54 - ETA: 3:23 - loss: 0.9232 - accuracy: 0.54 - ETA: 3:12 - loss: 0.9223 - accuracy: 0.54 - ETA: 3:00 - loss: 0.9243 - accuracy: 0.54 - ETA: 2:49 - loss: 0.9252 - accuracy: 0.54 - ETA: 2:38 - loss: 0.9259 - accuracy: 0.53 - ETA: 2:27 - loss: 0.9261 - accuracy: 0.53 - ETA: 2:17 - loss: 0.9235 - accuracy: 0.54 - ETA: 2:06 - loss: 0.9244 - accuracy: 0.54 - ETA: 1:55 - loss: 0.9254 - accuracy: 0.54 - ETA: 1:44 - loss: 0.9245 - accuracy: 0.54 - ETA: 1:33 - loss: 0.9259 - accuracy: 0.54 - ETA: 1:22 - loss: 0.9260 - accuracy: 0.54 - ETA: 1:12 - loss: 0.9261 - accuracy: 0.54 - ETA: 1:01 - loss: 0.9263 - accuracy: 0.54 - ETA: 50s - loss: 0.9260 - accuracy: 0.5423 - ETA: 39s - loss: 0.9258 - accuracy: 0.541 - ETA: 28s - loss: 0.9262 - accuracy: 0.542 - ETA: 17s - loss: 0.9262 - accuracy: 0.542 - ETA: 6s - loss: 0.9258 - accuracy: 0.542 - 427s 43ms/sample - loss: 0.9257 - accuracy: 0.5428 - val_loss: 1.0161 - val_accuracy: 0.5182\n",
      "Epoch 7/10\n",
      "9881/9881 [==============================] - ETA: 6:29 - loss: 0.9593 - accuracy: 0.54 - ETA: 6:38 - loss: 0.9126 - accuracy: 0.56 - ETA: 6:37 - loss: 0.9197 - accuracy: 0.55 - ETA: 6:30 - loss: 0.9267 - accuracy: 0.54 - ETA: 6:19 - loss: 0.9298 - accuracy: 0.54 - ETA: 6:13 - loss: 0.9264 - accuracy: 0.54 - ETA: 6:00 - loss: 0.9294 - accuracy: 0.53 - ETA: 5:47 - loss: 0.9296 - accuracy: 0.54 - ETA: 5:35 - loss: 0.9260 - accuracy: 0.54 - ETA: 5:22 - loss: 0.9208 - accuracy: 0.54 - ETA: 5:09 - loss: 0.9194 - accuracy: 0.54 - ETA: 4:57 - loss: 0.9180 - accuracy: 0.54 - ETA: 4:45 - loss: 0.9136 - accuracy: 0.54 - ETA: 4:33 - loss: 0.9128 - accuracy: 0.54 - ETA: 4:22 - loss: 0.9120 - accuracy: 0.54 - ETA: 4:09 - loss: 0.9100 - accuracy: 0.54 - ETA: 3:58 - loss: 0.9094 - accuracy: 0.54 - ETA: 3:48 - loss: 0.9089 - accuracy: 0.54 - ETA: 3:37 - loss: 0.9074 - accuracy: 0.54 - ETA: 3:26 - loss: 0.9133 - accuracy: 0.54 - ETA: 3:15 - loss: 0.9118 - accuracy: 0.54 - ETA: 3:05 - loss: 0.9114 - accuracy: 0.54 - ETA: 2:54 - loss: 0.9122 - accuracy: 0.54 - ETA: 2:43 - loss: 0.9120 - accuracy: 0.54 - ETA: 2:32 - loss: 0.9122 - accuracy: 0.54 - ETA: 2:21 - loss: 0.9132 - accuracy: 0.54 - ETA: 2:10 - loss: 0.9115 - accuracy: 0.54 - ETA: 1:59 - loss: 0.9114 - accuracy: 0.54 - ETA: 1:48 - loss: 0.9109 - accuracy: 0.54 - ETA: 1:37 - loss: 0.9099 - accuracy: 0.54 - ETA: 1:25 - loss: 0.9096 - accuracy: 0.54 - ETA: 1:14 - loss: 0.9103 - accuracy: 0.54 - ETA: 1:03 - loss: 0.9109 - accuracy: 0.54 - ETA: 52s - loss: 0.9090 - accuracy: 0.5476 - ETA: 40s - loss: 0.9096 - accuracy: 0.547 - ETA: 29s - loss: 0.9091 - accuracy: 0.547 - ETA: 18s - loss: 0.9093 - accuracy: 0.547 - ETA: 6s - loss: 0.9099 - accuracy: 0.547 - 441s 45ms/sample - loss: 0.9091 - accuracy: 0.5483 - val_loss: 0.9884 - val_accuracy: 0.5410\n",
      "Epoch 8/10\n",
      "9881/9881 [==============================] - ETA: 7:17 - loss: 0.8855 - accuracy: 0.57 - ETA: 6:49 - loss: 0.9327 - accuracy: 0.53 - ETA: 6:37 - loss: 0.9167 - accuracy: 0.56 - ETA: 6:20 - loss: 0.9152 - accuracy: 0.55 - ETA: 6:06 - loss: 0.9095 - accuracy: 0.55 - ETA: 6:00 - loss: 0.8990 - accuracy: 0.56 - ETA: 5:49 - loss: 0.8991 - accuracy: 0.56 - ETA: 5:35 - loss: 0.8992 - accuracy: 0.56 - ETA: 5:27 - loss: 0.8991 - accuracy: 0.56 - ETA: 5:14 - loss: 0.9031 - accuracy: 0.56 - ETA: 5:03 - loss: 0.9021 - accuracy: 0.55 - ETA: 4:53 - loss: 0.8979 - accuracy: 0.56 - ETA: 4:42 - loss: 0.9010 - accuracy: 0.55 - ETA: 4:33 - loss: 0.8987 - accuracy: 0.55 - ETA: 4:21 - loss: 0.8975 - accuracy: 0.55 - ETA: 4:09 - loss: 0.8977 - accuracy: 0.55 - ETA: 3:58 - loss: 0.8969 - accuracy: 0.55 - ETA: 3:46 - loss: 0.8947 - accuracy: 0.55 - ETA: 3:35 - loss: 0.8946 - accuracy: 0.55 - ETA: 3:25 - loss: 0.8952 - accuracy: 0.56 - ETA: 3:14 - loss: 0.8953 - accuracy: 0.56 - ETA: 3:03 - loss: 0.8940 - accuracy: 0.56 - ETA: 2:52 - loss: 0.8947 - accuracy: 0.56 - ETA: 2:41 - loss: 0.8941 - accuracy: 0.56 - ETA: 2:30 - loss: 0.8942 - accuracy: 0.56 - ETA: 2:20 - loss: 0.8938 - accuracy: 0.56 - ETA: 2:08 - loss: 0.8947 - accuracy: 0.56 - ETA: 1:57 - loss: 0.8958 - accuracy: 0.56 - ETA: 1:47 - loss: 0.8964 - accuracy: 0.56 - ETA: 1:36 - loss: 0.8949 - accuracy: 0.56 - ETA: 1:24 - loss: 0.8955 - accuracy: 0.56 - ETA: 1:13 - loss: 0.8986 - accuracy: 0.55 - ETA: 1:02 - loss: 0.8975 - accuracy: 0.56 - ETA: 51s - loss: 0.8964 - accuracy: 0.5609 - ETA: 40s - loss: 0.8974 - accuracy: 0.560 - ETA: 28s - loss: 0.8980 - accuracy: 0.560 - ETA: 17s - loss: 0.8973 - accuracy: 0.560 - ETA: 6s - loss: 0.8980 - accuracy: 0.559 - 435s 44ms/sample - loss: 0.8974 - accuracy: 0.5605 - val_loss: 0.9712 - val_accuracy: 0.5455\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9881/9881 [==============================] - ETA: 7:41 - loss: 0.8937 - accuracy: 0.55 - ETA: 7:20 - loss: 0.9042 - accuracy: 0.56 - ETA: 6:59 - loss: 0.8868 - accuracy: 0.58 - ETA: 6:42 - loss: 0.8916 - accuracy: 0.57 - ETA: 6:31 - loss: 0.8846 - accuracy: 0.57 - ETA: 6:20 - loss: 0.8733 - accuracy: 0.58 - ETA: 6:10 - loss: 0.8732 - accuracy: 0.58 - ETA: 5:59 - loss: 0.8706 - accuracy: 0.57 - ETA: 5:51 - loss: 0.8685 - accuracy: 0.57 - ETA: 5:38 - loss: 0.8722 - accuracy: 0.57 - ETA: 5:26 - loss: 0.8695 - accuracy: 0.57 - ETA: 5:13 - loss: 0.8662 - accuracy: 0.58 - ETA: 4:58 - loss: 0.8666 - accuracy: 0.57 - ETA: 4:46 - loss: 0.8664 - accuracy: 0.57 - ETA: 4:34 - loss: 0.8659 - accuracy: 0.57 - ETA: 4:22 - loss: 0.8671 - accuracy: 0.57 - ETA: 4:10 - loss: 0.8664 - accuracy: 0.57 - ETA: 3:57 - loss: 0.8662 - accuracy: 0.57 - ETA: 3:45 - loss: 0.8671 - accuracy: 0.57 - ETA: 3:33 - loss: 0.8692 - accuracy: 0.57 - ETA: 3:22 - loss: 0.8718 - accuracy: 0.57 - ETA: 3:10 - loss: 0.8746 - accuracy: 0.57 - ETA: 2:58 - loss: 0.8758 - accuracy: 0.57 - ETA: 2:47 - loss: 0.8770 - accuracy: 0.57 - ETA: 2:35 - loss: 0.8776 - accuracy: 0.57 - ETA: 2:24 - loss: 0.8783 - accuracy: 0.57 - ETA: 2:12 - loss: 0.8789 - accuracy: 0.57 - ETA: 2:01 - loss: 0.8784 - accuracy: 0.57 - ETA: 1:49 - loss: 0.8783 - accuracy: 0.57 - ETA: 1:38 - loss: 0.8790 - accuracy: 0.57 - ETA: 1:26 - loss: 0.8796 - accuracy: 0.57 - ETA: 1:15 - loss: 0.8788 - accuracy: 0.57 - ETA: 1:03 - loss: 0.8793 - accuracy: 0.57 - ETA: 52s - loss: 0.8779 - accuracy: 0.5757 - ETA: 41s - loss: 0.8789 - accuracy: 0.575 - ETA: 29s - loss: 0.8799 - accuracy: 0.574 - ETA: 18s - loss: 0.8781 - accuracy: 0.575 - ETA: 6s - loss: 0.8786 - accuracy: 0.574 - 445s 45ms/sample - loss: 0.8787 - accuracy: 0.5748 - val_loss: 0.9550 - val_accuracy: 0.5355\n",
      "Epoch 10/10\n",
      "9881/9881 [==============================] - ETA: 7:10 - loss: 0.8578 - accuracy: 0.61 - ETA: 7:04 - loss: 0.8259 - accuracy: 0.62 - ETA: 6:54 - loss: 0.8495 - accuracy: 0.60 - ETA: 6:42 - loss: 0.8552 - accuracy: 0.58 - ETA: 6:30 - loss: 0.8557 - accuracy: 0.58 - ETA: 6:16 - loss: 0.8552 - accuracy: 0.58 - ETA: 6:02 - loss: 0.8555 - accuracy: 0.58 - ETA: 5:51 - loss: 0.8598 - accuracy: 0.58 - ETA: 5:39 - loss: 0.8660 - accuracy: 0.58 - ETA: 5:28 - loss: 0.8691 - accuracy: 0.58 - ETA: 5:16 - loss: 0.8702 - accuracy: 0.57 - ETA: 5:04 - loss: 0.8690 - accuracy: 0.58 - ETA: 4:50 - loss: 0.8758 - accuracy: 0.57 - ETA: 4:39 - loss: 0.8786 - accuracy: 0.57 - ETA: 4:27 - loss: 0.8782 - accuracy: 0.56 - ETA: 4:17 - loss: 0.8787 - accuracy: 0.57 - ETA: 4:05 - loss: 0.8803 - accuracy: 0.57 - ETA: 3:54 - loss: 0.8805 - accuracy: 0.57 - ETA: 3:43 - loss: 0.8813 - accuracy: 0.57 - ETA: 3:32 - loss: 0.8819 - accuracy: 0.57 - ETA: 3:20 - loss: 0.8834 - accuracy: 0.57 - ETA: 3:08 - loss: 0.8832 - accuracy: 0.57 - ETA: 2:57 - loss: 0.8824 - accuracy: 0.57 - ETA: 2:44 - loss: 0.8826 - accuracy: 0.56 - ETA: 2:30 - loss: 0.8816 - accuracy: 0.57 - ETA: 2:17 - loss: 0.8820 - accuracy: 0.57 - ETA: 2:04 - loss: 0.8809 - accuracy: 0.57 - ETA: 1:52 - loss: 0.8772 - accuracy: 0.57 - ETA: 1:40 - loss: 0.8774 - accuracy: 0.57 - ETA: 1:29 - loss: 0.8781 - accuracy: 0.57 - ETA: 1:18 - loss: 0.8769 - accuracy: 0.57 - ETA: 1:07 - loss: 0.8760 - accuracy: 0.57 - ETA: 56s - loss: 0.8776 - accuracy: 0.5726 - ETA: 45s - loss: 0.8764 - accuracy: 0.573 - ETA: 35s - loss: 0.8748 - accuracy: 0.573 - ETA: 25s - loss: 0.8750 - accuracy: 0.574 - ETA: 15s - loss: 0.8743 - accuracy: 0.575 - ETA: 5s - loss: 0.8732 - accuracy: 0.576 - 371s 38ms/sample - loss: 0.8741 - accuracy: 0.5755 - val_loss: 0.9083 - val_accuracy: 0.5665\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "hist = model.fit([data],  np.array(train_y), \n",
    "                 validation_split=0.1, \n",
    "                 epochs=epochs, \n",
    "                 batch_size=batch_size, \n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_x,np.array(test_y))\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'positive', ..., 'neutral', 'positive',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.DataFrame(hist.history)\n",
    "plt.figure(figsize=(12,12));\n",
    "plt.plot(history[\"loss\"]);\n",
    "plt.plot(history[\"val_loss\"]);\n",
    "plt.title(\"Loss with pretrained word vectors\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chart_studio        #Install chart_studio for plotly plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py                      #Import chart_studio for various plotly plot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import iplot\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'colab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot shows the Bi-LSTM ML Classification model for Evaluation metrics of Negative sentiment label\n",
    "trace1 = {\n",
    "  \"name\": \"Accuracy\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [57]\n",
    "}\n",
    "\n",
    "trace2 = {\n",
    "  \"name\": \"Precision\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [66]\n",
    "}\n",
    "\n",
    "trace3 = {\n",
    "  \"name\": \"Recall\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [16]\n",
    "}\n",
    "\n",
    "trace4 = {\n",
    "  \"name\": \"F1-score\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [25]\n",
    "}\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4]\n",
    "layout = go.Layout(barmode = \"group\",title= 'BI-LSTM ML Model Evaluation Metrics Comparision on Negative Tweet sentiment ')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot shows the Bi-LSTM ML Classification model for Evaluation metrics of Neutral sentiment label\n",
    "trace1 = {\n",
    "  \"name\": \"Accuracy\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [57]\n",
    "}\n",
    "\n",
    "trace2 = {\n",
    "  \"name\": \"Precision\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [44]\n",
    "}\n",
    "\n",
    "trace3 = {\n",
    "  \"name\": \"Recall\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [86]\n",
    "}\n",
    "\n",
    "trace4 = {\n",
    "  \"name\": \"F1-score\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [58]\n",
    "}\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4]\n",
    "layout = go.Layout(barmode = \"group\",title= 'BI-LSTM ML Model Evaluation Metrics Comparision on Neutral Tweet sentiment ')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot shows the Bi-LSTM ML Classification model for Evaluation metrics of Positive sentiment label\n",
    "trace1 = {\n",
    "  \"name\": \"Accuracy\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [57]\n",
    "}\n",
    "\n",
    "trace2 = {\n",
    "  \"name\": \"Precision\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [56]\n",
    "}\n",
    "\n",
    "trace3 = {\n",
    "  \"name\": \"Recall\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [33]\n",
    "}\n",
    "\n",
    "trace4 = {\n",
    "  \"name\": \"F1-score\", \n",
    "  \"type\": \"bar\", \n",
    "  \"x\": [\"Bi-LSTM\"], \n",
    "  \"y\": [42]\n",
    "}\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4]\n",
    "layout = go.Layout(barmode = \"group\",title= 'BI-LSTM ML Model Evaluation Metrics Comparision on Positive Tweet sentiment ')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
